 ▲                                                   ▲
 █ The ~> 4.0 syntax is equivalent to >= 4.0, < 5.0. █
 ▼                                                   ▼

There are four core values in the DevOps movement: culture, automation, measure‐
  ment, and sharing (sometimes abbreviated as the acronym CAMS). 

What Is Infrastructure as Code?:
The idea behind infrastructure as code (IaC) is that you write and execute code to
  define, deploy, update, and destroy your infrastructure. This represents an important
  shift in mindset in which you treat all aspects of operations as software—even those
  aspects that represent hardware (e.g., setting up physical servers). In fact, a key
  insight of DevOps is that you can manage almost everything in code, including
  servers, databases, networks, logfiles, application configuration, documentation, auto‐
  mated tests, deployment processes, and so on.

There are five broad categories of IaC tools:
• Ad hoc scripts
• Configuration management tools
• Server templating tools
• Orchestration tools
• Provisioning tools

The great thing about ad hoc scripts is that you can use popular, general-purpose
  programming languages, and you can write the code however you want. The terrible
  thing about ad hoc scripts is that you can use popular, general-purpose programming
  languages, and you can write the code however you want.


Configuration management tools: 
using a tool like Ansible offers a number of advantages:

Coding conventions
  Ansible enforces a consistent, predictable structure, including documentation, file layout, 
  clearly named parameters, secrets management, and so on. While every developer organizes 
  their ad hoc scripts in a different way, most configuration management tools come with a 
  set of conventions that makes it easier to navigate the code.
  
Idempotence
  Writing an ad hoc script that works once isn’t too difficult; writing an ad hoc script that works 
  correctly even if you run it over and over again is much harder. Every time you go to create a 
  folder in your script, you need to remember to check whether that folder already exists; every 
  time you add a line of configuration to a file, you need to check that line doesn’t already exist; 
  every time you want to run an app, you need to check that the app isn’t already running. Code that 
  works correctly no matter how many times you run it is called idempotent code. To make the Bash 
  script from the previous section idempotent, you’d need to add many lines of code, including lots 
  of if-statements. Most Ansible functions, on the other hand, are idempotent by default. For example, 
  the web-server.yml Ansible role will install Apache only if it isn’t installed already and
  will try to start the Apache web server only if it isn’t running already.
  
Distribution
  Ad hoc scripts are designed to run on a single, local machine. Ansible and other configuration 
  management tools are designed specifically for managing large numbers of remote servers



Server Templating Tools
  An alternative to configuration management that has been growing in popularity recently are 
  server templating tools such as Docker, Packer, and Vagrant. Instead of launching a bunch of 
  servers and configuring them by running the same code on each one, the idea behind server templating 
  tools is to create an image of a server that captures a fully self-contained “snapshot” of the 
  operating system (OS), the software, the files, and all other relevant details. You can then use 
  some other IaC tool to install that image on all of your servers


On most modern operating systems, code runs in one of two “spaces”: kernel space or user space. Code running 
  in kernel space has direct, unrestricted access to all of the hardware. There are no security restrictions
  (i.e., you can execute any CPU instruction, access any part of the hard drive, write to any address in memory)
  or safety restrictions (e.g., a crash in kernel space will typically crash the entire computer), so kernel space
  is generally reserved for the lowest-level, most trusted functions of the OS (typically called the kernel). Code
  running in user space does not have any direct access to the hardware and must use APIs exposed by the OS
  kernel instead. These APIs can enforce security restrictions (e.g., user permissions) and safety (e.g., a crash in
  a user space app typically affects only that app), so just about all application code runs in user space

Note that the different server templating tools have slightly different purposes. Packer is typically 
  used to create images that you run directly on top of production servers, such as an AMI that you 
  run in your production AWS account. Vagrant is typically used to create images that you run on 
  your development computers, such as a VirtualBox image that you run on your Mac or Windows laptop. 
  Docker is typically used to create images of individual applications. You can run the Docker images on
  production or development computers, as long as some other tool has configured that computer with the 
  Docker Engine. For example, a common pattern is to use Packer to create an AMI that has the Docker 
  Engine installed, deploy that AMI on a cluster of servers in your AWS account, and then deploy 
  individual Docker containers across that cluster to run your applications.

The idea behind immutable infrastructure is similar: once you’ve deployed a server, you never make 
  changes to it again. If you need to update something, such as deploying a new version of your code, 
  you create a new image from your server template and you deploy it on a new server. Because 
  servers never change, it’s a lot easier to reason about what’s deployed.
  
According to the 2016 State of DevOps Report, organizations that use DevOps practices, such as 
  IaC, deploy 200 times more frequently, recover from failures 24 times 
  faster, and have lead times that are 2,555 times lower.


Highlights two major problems with procedural IaC tools:
Procedural code does not fully capture the state of the infrastructure
  Reading through the three preceding Ansible templates is not enough to know what’s deployed. You’d 
  also need to know the order in which those templates were applied. Had you applied them in a different 
  order, you might have ended up with different infrastructure, and that’s not something you can see 
  in the codebase itself. In other words, to reason about an Ansible or Chef codebase, you need 
  to know the full history of every change that has ever happened.
Procedural code limits reusability
  The reusability of procedural code is inherently limited because you must man‐
  ually take into account the current state of the infrastructure. Because that state
  is constantly changing, code you used a week ago might no longer be usable
  because it was designed to modify a state of your infrastructure that no longer
  exists. As a result, procedural codebases tend to grow large and complicated over time.


General-Purpose Language Versus Domain-Specific Language
  Chef and Pulumi allow you to use a general-purpose programming language (GPL) to manage infrastructure 
  as code: Chef supports Ruby; Pulumi supports a wide variety of GPLs, including JavaScript, TypeScript, 
  Python, Go, C#, Java, and others. Terraform, Puppet, Ansible, CloudFormation, and OpenStack Heat each 
  use a domain-specific language (DSL) to manage infrastructure as code: Terraform uses HCL; Puppet uses 
  Puppet Language; Ansible, CloudFormation, and OpenStack Heat use YAML (CloudFormation also supports JSON).



Master Versus Masterless
  By default, Chef and Puppet require that you run a master server for storing the state of your 
  infrastructure and distributing updates. Every time you want to update something in your 
  infrastructure, you use a client (e.g., a command-line tool) to issue new commands to the master 
  server, and the master server either pushes the updates out to all of the other servers or those 
  servers pull the latest updates down from the master server on a regular basis.
  
Having to run a master server has some serious drawbacks:
Extra infrastructure
  You need to deploy an extra server, or even a cluster of extra servers (for high
  availability and scalability), just to run the master.
  
Maintenance
  You need to maintain, upgrade, back up, monitor, and scale the master server(s).
  
Security
  You need to provide a way for the client to communicate to the master server(s) and a way for the 
  master server(s) to communicate with all the other servers, which typically means opening extra ports 
  and configuring extra authentication systems, all of which increases your surface area to attacker


Ansible, CloudFormation, Heat, Terraform, and Pulumi are all masterless by default. Or, to be 
  more accurate, some of them rely on a master server, but it’s already part of the infrastructure 
  you’re using and not an extra piece that you need to manage. For example, Terraform communicates 
  with cloud providers using the cloud provider’s APIs, so in some sense, the API servers are master 
  servers, except that they don’t require any extra infrastructure or any extra authentication 
  mechanisms (i.e., just use your API keys). Ansible works by connecting directly to each server over 
  SSH, so again, you don’t need to run any extra infrastructure or manage extra authentication
  mechanisms (i.e., just use your SSH keys).



Agent Versus Agentless:
  Chef and Puppet require you to install agent software (e.g., Chef Client, Puppet Agent) on each 
  server that you want to configure. The agent typically runs in the background on each server and 
  is responsible for installing the latest configuration management updates. This has a few drawbacks:
Bootstrapping
  How do you provision your servers and install the agent software on them in the first place? 
  Some configuration management tools kick the can down the road, assuming that some external process 
  will take care of this for them (e.g., you first use Terraform to deploy a bunch of servers with 
  an AMI that has the agent already installed); other configuration management tools have a special
  bootstrapping process in which you run one-off commands to provision the servers using the cloud 
  provider APIs and install the agent software on those servers over SSH.
Maintenance
  You need to update the agent software on a periodic basis, being careful to keep it 
  synchronized with the master server if there is one. You also need to monitor the
  agent software and restart it if it crashes.
Security
  If the agent software pulls down configuration from a master server (or some other server if 
  you’re not using a master), you need to open outbound ports on every server. If the master server 
  pushes configuration to the agent, you need to open inbound ports on every server. In either case, 
  you must figure out how to authenticate the agent to the server to which it’s communicating. 
  All of this increases your surface area to attackers.


A Note on Default Virtual Private Clouds
  All of the AWS examples in this book use the Default VPC in your AWS account. A VPC, or virtual 
  private cloud, is an isolated area of your AWS account that has its own virtual network and IP
  address space. Just about every AWS resource deploys into a VPC. If you don’t explicitly specify 
  a VPC, the resource will be deployed into the Default VPC, which is part of every AWS account created
  after 2013. If for some reason you deleted the Default VPC in your account, either use a different 
  region (each region has its own Default VPC) or create a new Default VPC using the AWS Web
  Console. Otherwise, you’ll need to update almost every example
  to include a vpc_id or subnet_id parameter pointing to a custom VPC.


run terraform init to tell Terraform to scan the code, figure out which providers
  you’re using, and download the code for them. By default, the provider code will
  be downloaded into a .terraform folder, which is Terraform’s scratch directory (you
  may want to add it to .gitignore). Terraform will also record information about the
  provider code it downloaded into a .terraform.lock.hcl file

The -/+ in the plan output means “replace”; look for the text “forces replacement”
  in the plan output to figure out what is forcing Terraform to do a replacement.

The body of the variable declaration can contain the following optional parameters:
description
  It’s always a good idea to use this parameter to document how a variable is used.
  Your teammates will be able to see this description not only while reading the
  code but also when running the plan or apply commands 
  
default
  There are a number of ways to provide a value for the variable, including passing it in at the 
  command line (using the -var option), via a file (using the -var- file option), or via an 
  environment variable (Terraform looks for environment variables of the name TF_VAR_<variable_name>). 
  If no value is passed in, the variable will fall back to this default value. If there is no 
  default value, Terraform will interactively prompt the user for one.
  
type
  This allows you to enforce type constraints on the variables a user passes in. Terraform supports 
  a number of type constraints, including string, number, bool, list, map, set, object, tuple, and any. 
  It’s always a good idea to define a type constraint to catch simple errors. If you don’t 
  specify a type, Terraform assumes the type is any.
  
validation
  This allows you to define custom validation rules for the input variable that go
  beyond basic type checks, such as enforcing minimum or maximum values on a number .
  
sensitive
  If you set this parameter to true on an input variable, Terraform will not log it
  when you run plan or apply. You should use this on any secrets you pass into
  your Terraform code via variables: e.g., passwords, API keys, etc. 


In addition to input variables, Terraform also allows you to define output variables
  The CONFIG can contain the following optional parameters:

description
  It’s always a good idea to use this parameter to document what type of data is
  contained in the output variable.
sensitive
  Set this parameter to true to instruct Terraform not to log this output at the end of plan or 
  apply. This is useful if the output variable contains secrets such as passwords or private keys. 
  Note that if your output variable references an input variable or resource attribute marked with 
  sensitive = true, you are required to mark the output variable with sensitive = true as well to 
  indicate you are intentionally outputting a secret.
depends_on
  Normally, Terraform automatically figures out your dependency graph based
  on the references within your code, but in rare situations, you have to give it
  extra hints. For example, perhaps you have an output variable that returns the
  IP address of a server, but that IP won’t be accessible until a security group
  (firewall) is properly configured for that server. In that case, you may explicitly
  tell Terraform there is a dependency between the IP address output variable and
  the security group resource using depends_on.


AWS offers three types of load balancers:
Application Load Balancer (ALB)
  Best suited for load balancing of HTTP and HTTPS traffic. Operates at the
  application layer (Layer 7) of the Open Systems Interconnection (OSI) model.
  
Network Load Balancer (NLB)
  Best suited for load balancing of TCP, UDP, and TLS traffic. Can scale up and down in 
  response to load faster than the ALB (the NLB is designed to scale to tens of millions 
  of requests per second). Operates at the transport layer (Layer 4) of the OSI model.
  
Classic Load Balancer (CLB)
  This is the “legacy” load balancer that predates both the ALB and NLB. It can handle 
  HTTP, HTTPS, TCP, and TLS traffic but with far fewer features than either the ALB or NLB. 
  Operates at both the application layer (L7) and transport layer (L4) of the OSI model.


The ALB consists of several parts: 
Listener
  Listens on a specific port (e.g., 80) and protocol (e.g., HTTP)
  
Listener rule
  Takes requests that come into a listener and sends those that match specific paths (e.g., /foo and /bar) 
  or hostnames (e.g., foo.example.com and bar.exam ple.com) to specific target groups.

Target groups
  One or more servers that receive requests from the load balancer. The target
  group also performs health checks on these servers and sends requests only to healthy nodes.


There are two ways you could isolate state files:
Isolation via workspaces: 
  Useful for quick, isolated tests on the same configuration
  
Isolation via file layout:
  Useful for production use cases for which you need strong separation between environments


Workspaces drawbacks:
• The state files for all of your workspaces are stored in the same backend (e.g., the same S3 bucket). 
  That means you use the same authentication and access controls for all the workspaces, which is one 
  major reason workspaces are an unsuitable mechanism for isolating environments 
  (e.g., isolating staging from production).
  
• Workspaces are not visible in the code or on the terminal unless you run terra form workspace commands. 
  When browsing the code, a module that has been deployed in one workspace looks exactly the same as a 
  module deployed in 10 workspaces. This makes maintenance more difficult, because you don’t have a
  good picture of your infrastructure.
  
• Putting the two previous items together, the result is that workspaces can be fairly error prone. The lack 
  of visibility makes it easy to forget what workspace you’re in and accidentally deploy changes in the wrong 
  one (e.g., accidentally running terraform destroy in a “production” workspace rather than a “staging”
  workspace), and because you must use the same authentication mechanism for
  all workspaces, you have no other layers of defense to protect against such errors.


Isolation via File Layout
To achieve full isolation between environments, you need to do the following:
• Put the Terraform configuration files for each environment into a separate folder. For example, all 
  of the configurations for the staging environment can be in a folder called stage and all the 
  configurations for the production environment can be in a folder called prod.
  
• Configure a different backend for each environment, using different authentication mechanisms and access 
  controls: e.g., each environment could live in a separate AWS account with a separate S3 bucket as a backend.


terraform typical folder structure: 
stage
  An environment for pre-production workloads (i.e., testing)
  
prod
  An environment for production workloads (i.e., user-facing apps)
  
mgmt
  An environment for DevOps tooling (e.g., bastion host, CI server)
  
global
  A place to put resources that are used across all environments (e.g., S3, IAM) 
  Within each environment, there are separate folders for each “component.” The components differ for 
  every project, but here are the typical ones:
    vpc
      The network topology for this environment.
    services
      The apps or microservices to run in this environment, such as a Ruby on Rails frontend or a Scala 
      backend. Each app could even live in its own folder to isolate it from all the other apps.
    data-storage
      The data stores to run in this environment, such as MySQL or Redis. Each data
      store could even reside in its own folder to isolate it from all other data stores.
      Within each component, there are the actual Terraform configuration files, which are
      organized according to the following naming conventions:
        variables.tf
          Input variables
        outputs.tf
          Output variables
        main.tf
          Resources and data sources



terraform typical files names: 
dependencies.tf
  It’s common to put all your data sources in a dependencies.tf file to make it easier
  to see what external things the code depends on.
  
providers.tf
  You may want to put your provider blocks into a providers.tf file so you can see, at a glance, 
  what providers the code talks to and what authentication you’ll have to provide.
  
main-xxx.tf
  If the main.tf file is getting really long because it contains a large number of resources, you could 
  break it down into smaller files that group the resources in some logical way: e.g., main-iam.tf 
  could contain all the IAM resources, main- s3.tf could contain all the S3 resources, and so on. 
  Using the main- prefix makes it easier to scan the list of files in a folder when they are 
  organized alphabetically, as all the resources will be grouped together. It’s also worth noting that 
  if you find yourself managing a very large number of resources and struggling to break them down across 
  many files, that might be a sign that you should break your code into smaller modules instead,


Cleaning Up After Tests: 
  a common pattern is to run cloud-nuke as a cron job once per day in each sandbox environment to delete all 
  resources that are more than 48 hours old, based on the assumption that any infrastructure a developer fired 
  up for manual testing is no longer necessary after a couple of days:
      $ cloud-nuke aws --older-than 48h

The basic strategy for writing unit tests for Terraform is as follows:
  1. Create a small, standalone module.
  2. Create an easy-to-deploy example for that module.
  3. Run terraform apply to deploy the example into a real environment.
  4. Validate that what you just deployed works as expected. This step is specific to
     the type of infrastructure you’re testing: for example, for an ALB, you’d validate
     it by sending an HTTP request and checking that you receive back the expected response.
  5. Run terraform destroy at the end of the test to clean up.



For example, you can use tools such as tfsec and tflint to enforce policies, such as:
• Security groups cannot be too open: e.g., block inbound rules that allow access
  from all IPs (CIDR block 0.0.0.0/0).
• All EC2 Instances must follow a specific tagging convention.




