The DevOps process is divided into several phases:
  A. Planning and prioritizing functionalities
  B. Development
  C. Continuous integration and delivery
  D. Continuous deployment
  E. Continuous monitoring

CI is an automatic process that allows you to check the completeness of an application's code every 
  time a team member makes a change. This verification must be done as quickly as possible.

Unlike CI, CD aims to test the entire application with all of its dependencies. This is very
  visible in microservice applications composed of several services and APIs; CI will only
  test the microservice under development, while once deployed in a staging environment,
  it will be possible to test and validate the entire application, as well as the APIs and
  microservices that it is composed of.

It is important that the package that's generated during CI, which will also be deployed
  during CD, is the same one that will be installed on all environments, and this should be
  the case until production. However, there may be configuration file transformations that
  differ, depending on the environment, but the application code (binaries, DLL, Docker
  images, and JAR) must remain unchanged.

-- Once continuous integration has been completed, the next step is to deploy the application automatically in 
   one or more non-production environments, which is called staging. This process is called continuous delivery (CD).

-- CD often starts with an application package being prepared by CI, which will be installed 
   based on a list of automated tasks. These tasks can be of any type: unzip, stop and restart 
   service, copy files, replace configuration, and so on. The execution of functional and
   acceptance tests can also be performed during the CD process.

The tools that are set up for CI/CD are often used with other solutions, as follows:
• A package manager: This constitutes the storage space of the packages generated by CI and recovered by CD. 
    These managers must support feeds, versioning, and different types of packages. There are several 
    on the market, such as Nexus, ProGet, Artifactory, and Azure Artifacts.
• A configuration manager: This allows you to manage configuration changes during
    CD; most CD tools include a configuration mechanism with a system of variables.

In CD, deploying the application in each staging environment is triggered as follows:
• It can be triggered automatically, following a successful execution in a previous
  environment. For example, we can imagine a case where the deployment in the pre-production environment is 
  automatically triggered when the integration tests have been successfully performed in a dedicated environment.
• It can be triggered manually, for sensitive environments such as the production environment, following manual 
  approval by the person responsible for validating the proper functionality of the application in an environment.

Continuous deployment
  Continuous deployment is an extension of CD, but this time, with a process that automates the entire CI/CD pipeline 
  from the moment the developer commits their code to deployment in production through all of the verification steps.

Continuous deployment can be implemented by using and implementing feature toggle techniques (or feature flags), 
  which involves encapsulating the application's functionalities in features and activating its features on 
  demand, directly in production, without having to redeploy the code of the application.
Another technique is to use a blue-green production infrastructure, which consists
  of two production environments, one blue and one green. First, we deploy to the blue
  environment, then to the green one; this will ensure that no downtime is required.



The benefits of IaC are as follows:
  • The standardization of infrastructure configuration reduces the risk of errors.
  • The code that describes the infrastructure is versioned and controlled in a source code manager.
  • The code is integrated into CI/CD pipelines.
  • Deployments that make infrastructure changes are faster and more efficient.
  • There's better management, control, and a reduction in infrastructure costs.


-- Provisioning is the act of instantiating the resources that make up the infrastructure. They
    can be of the Platform-as-a-Service (PaaS) and serverless resource types, such as a web
    app, Azure function, or Event Hub, but also the entire network part that is managed, such
    as VNet, subnets, routing tables, or Azure Firewall. For virtual machine resources, the
    provisioning step only creates or updates the VM cloud resource, but not its content.


Server configuration
  This step concerns configuring virtual machines, such as the hardening, directories, disk
  mounting, network configuration (firewall, proxy, and so on), and middleware installation
  There are different configuration tools, such as Ansible, PowerShell DSC, Chef, Puppet, and SaltStack


Protecting the state file with a remote backend
  When Terraform handles resources, it writes the state of these resources in a Terraform state file. This 
  file is in JSON format and preserves the resources and their properties throughout the execution of Terraform.
  By default, this file, called terraform.tfstate, is created locally when the first
  execution of the apply command is executed. Then, it will be used by Terraform each time
  the plan command is executed in order to compare its state (written in the state file) with
  that of the target infrastructure. Finally, it will return a preview of what will be applied

  When using Terraform in an enterprise, this locally stored state file poses many problems:
  • Knowing that this file contains the status of the infrastructure, it should not be
    deleted. If deleted, Terraform might not behave as expected when it is executed.
  • It must be accessible at the same time by all members of the team who are handling resources on the same infrastructure.
  • This file can contain sensitive data, so it must be secure.
  • When provisioning multiple environments, it is necessary to be able to use multiple state files.

  With all of these points, it is not possible to keep this state file locally or 
  even to archive it in an SCM. To solve this problem, Terraform allows this state 
  file to be stored in a shared and secure storage called the remote backend.

  In our case, we will use an azurerm remote backend to store our state files with a storage account and a blob 
  for the state file. Therefore, we will implement and use a remote backend in three steps:
 1. The creation of the storage account
 2. The Terraform configuration for the remote backend
 3. The execution of Terraform with the use of this remote backend

if multiple Terraform states are used to manage multiple environments,
it's possible to create several remote backend configurations with the simplified code in the .tf file:
╒══════════════════════╕
  terraform {          
     backend "azurerm" {} 
  }                    
╘══════════════════════╛
 
