
A few of the characteristics of cloud native systems that most people can agree on:
  Automatable
      If applications are to be deployed and managed by machines, instead of humans,
      they need to abide by common standards, formats, and interfaces. Kubernetes
      provides these standard interfaces in a way that means application developers
      don’t even need to worry about them.
      
  Ubiquitous and flexible
      Because they are decoupled from physical resources such as disks, or any specific
      knowledge about the compute node they happen to be running on, containerized
      microservices can easily be moved from one node to another, or even one cluster to another.
      
  Resilient and scalable
      Traditional applications tend to have single points of failure: the application stops
      working if its main process crashes, or if the underlying machine has a hardware failure, 
      or if a network resource becomes congested. Cloud native applications, because they are inherently 
      distributed, can be made highly available through redundancy and graceful degradation.
  
  Dynamic
      A container orchestrator such as Kubernetes can schedule containers to take
      maximum advantage of available resources. It can run many copies of them to
      achieve high availability, and perform rolling updates to smoothly upgrade serv‐
      ices without ever dropping traffic.

  Observable
      Cloud native apps, by their nature, are harder to inspect and debug. So a key
      requirement of distributed systems is observability: monitoring, logging, tracing,
      and metrics all help engineers understand what their systems are doing (and what they’re doing wrong).

  Distributed
      Cloud native is an approach to building and running applications that takes
      advantage of the distributed and decentralized nature of the cloud. It’s about how
      your application works, not where it runs. Instead of deploying your code as a single entity 
      (known as a monolith), cloud native applications tend to be composed of multiple, cooperating, 
      distributed microservices. A microservice is simply a self-contained service that does one thing. 
      If you put enough microservices together, you get an application.



The control plane is actually made up of several components:
    kube-apiserver
      This is the frontend server for the control plane, handling API requests.
    etcd
      This is the database where Kubernetes stores all its information: what nodes exist,
      what resources exist on the cluster, and so on.
    kube-scheduler
      This decides where to run newly created Pods.
    kube-controller-manager
      This is responsible for running resource controllers, such as Deployments.
    cloud-controller-manager
      This interacts with the cloud provider (in cloud-based clusters), managing
      resources such as load balancers and disk volumes.
    The members of the cluster which run the control plane components are called master nodes.


Node Components
  Cluster members that run user workloads are called worker nodes 
  Each worker node in a Kubernetes cluster runs these components:
    kubelet
      This is responsible for driving the container runtime to start workloads that are
      scheduled on the node, and monitoring their status.
    kube-proxy
      This does the networking magic that routes requests between Pods on different
      nodes, and between Pods and the internet.
    Container runtime
      This actually starts and stops containers and handles their communications. Usu‐
      ally Docker, but Kubernetes supports other container runtimes, such as rkt and CRI-O.

Other than running different software components, there’s no intrinsic difference
  between master nodes and worker nodes. Master nodes don’t usually run user work-loads, 
  though, except in very small clusters (like Docker Desktop or Minikube).

The etcd database is replicated across multiple nodes, and can survive the failure of individual nodes, 
  so long as a quorum of over half the original number of etcd replicas is still available.


• Kubernetes clusters are made up of master nodes, which run the control plane,
    and worker nodes, which run your workloads.
• Production clusters must be highly available, meaning that the failure of a master
    node won’t lose data or affect the operation of the cluster.
• It’s a long way from a simple demo cluster to one that’s ready for critical produc‐
    tion workloads. High availability, security, and node management are just some
    of the issues involved.
• Managing your own clusters requires a significant investment of time, effort, and
    expertise. Even then, you can still get it wrong.
• Managed services like Google Kubernetes Engine do all the heavy lifting for you,
    at much lower cost than self-hosting.
• Turnkey services are a good compromise between self-hosted and fully managed
    Kubernetes. Turnkey providers like Stackpoint manage the master nodes for you,
    while you run worker nodes on your own machines.
• If you have to host your own cluster, kops is a mature and widely used tool that
    can provision and manage production-grade clusters on AWS and Google Cloud.
• You should use managed Kubernetes if you can. This is the best option for most
    businesses in terms of cost, overhead, and quality.
• If managed services aren’t an option, consider using turnkey services as a good compromise.
• Don’t self-host your cluster without sound business reasons. If you do self-host, don’t underestimate 
    the engineering time involved for the initial setup and ongo‐ ing maintenance overhead.


Working together with the Deployment resource is a kind of Kubernetes object called
  a controller. Controllers watch the resources they’re responsible for, making sure
  they’re present and working. If a given Deployment isn’t running enough replicas, for
  whatever reason, the controller will create some new ones. (If there were too many
  replicas for some reason, the controller would shut down the excess ones. Either way,
  the controller makes sure that the real state matches the desired state.)


ReplicaSets
  A ReplicaSet is responsible for a group of identical Pods, or replicas. If there are too
  few (or too many) Pods, compared to the specification, the ReplicaSet controller will start (or stop) 
  some Pods to rectify the situation. Deployments, in turn, manage ReplicaSets, and control how the replicas 
  behave when you update them—by rolling out a new version of your application, for example  When you 
  update the Deployment, a new ReplicaSet is created to manage the new Pods, and when the update 
  is completed, the old ReplicaSet and its Pods are terminated.


When a Deployment (via its associated ReplicaSet) decides that a new replica is
    needed, it creates a Pod resource in the Kubernetes database. Simultaneously, this Pod is added 
    to a queue, which is like the scheduler’s inbox. The scheduler’s job is to watch its queue of 
    unscheduled Pods, grab the next Pod from it, and find a node to run it on. It will use a few 
    different criteria, including the Pod’s resource requests, to choose a suitable node, assuming 
    there is one available Once the Pod has been scheduled on a node, the kubelet running on that 
    node picks it up and takes care of actually starting its containers 
    (What would happen if you shut the node down altogether? Its Pods would become unscheduled and
     go back into the scheduler’s queue, to be reassigned to other nodes.)

--  all the kubectl run command does is add a new record in the database corresponding 
    to a Deployment, and Kubernetes does the rest.



These are the three most important Helm terms you need to know:
  • A chart is a Helm package, containing all the resource definitions necessary to
    run an application in Kubernetes.
  • A repository is a place where charts can be collected and shared.
  • A release is a particular instance of a chart running in a Kubernetes cluster.

    One chart can often be installed many times into the same cluster. For example, you
    might be running multiple copies of the Nginx web server chart, each serving a different 
    site. Each separate instance of the chart is a distinct release.
    Each release has a unique name, which you specify with the -name flag to helm install.


  
• The Pod is the fundamental unit of work in Kubernetes, specifying a single container or group
    of communicating containers that are scheduled together.
• A Deployment is a high-level Kubernetes resource that declaratively manages
    Pods, deploying, scheduling, updating, and restarting them when necessary.
• A Service is the Kubernetes equivalent of a load balancer or proxy, routing traffic
    to its matching Pods via a single, well-known, durable IP address or DNS name.
• The Kubernetes scheduler watches for a Pod that isn’t yet running on any node,
    finds a suitable node for it, and instructs the kubelet on that node to run the Pod.
• Resources like Deployments are represented by records in Kubernetes’s internal
    database. Externally, these resources can be represented by text files (known as
    manifests) in YAML format. The manifest is a declaration of the desired state of the resource.
• kubectl is the main tool for interacting with Kubernetes, allowing you to apply
    manifests, query resources, make changes, delete resources, and do many other tasks.
• Helm is a Kubernetes package manager. It simplifies configuring and deploying
    Kubernetes applications, allowing you to use a single set of values (such as the
    application name or listen port) and a set of templates to generate Kubernetes
    YAML files, instead of having to maintain the raw YAML files yourself.


a resource request of 100m (100 millicpus) and
  250Mi (250 MiB of memory) means that the Pod cannot be scheduled on a node with
  less than those resources available. If there isn’t any node with enough capacity available, 
  the Pod will remain in a pending state until there is.


Alternatively, you could have the application create a file on the container’s filesystem
    called something like /tmp/healthy, and use an exec readiness probe to check for the
    presence of that file.
    This kind of readiness probe can be useful, because if you want to take the container temporarily out 
    of service to debug a problem, you can attach to the container and delete the /tmp/healthy file. The next 
    readiness probe will fail, and Kubernetes will remove the container from any matching Services.


By default, a container or Pod is considered ready the moment its readiness probe
    succeeds. In some cases, you may want to run the container for a short while to make sure it is stable. 
    During a deployment, Kubernetes waits until each new Pod is ready before starting the next. If a faulty 
    container crashes straight away, this will halt the rollout, but if it takes a few seconds to crash,
    all its replicas might be rolled out before you discover the problem.
    To avoid this, you can set the minReadySeconds field on the container. A container or Pod will 
    not be considered ready until its readiness probe has been up for minReady Seconds (default 0).


-- You can use the PodDisruptionBudget resource to specify, for a given application, 
    how many Pods you can afford to lose at any given time.

-- A Kubernetes namespace is a way of partitioning your cluster into separate subdivisions, for 
   whatever purpose you like. For example, you might have a prod namespace for production 
   applications, and a test namespace for trying things out.


-- In general, you should set the resource limits for a container to a little above the 
   maximum it uses in normal operation. For example, if a given container’s memory usage over 
   a few days never exceeds 500 MiB of memory, you might set its memory limit to 600 MiB.

-- Vertical Pod Autoscaler
      There is a Kubernetes add-on called the Vertical Pod Autoscaler, which can help you
      work out the ideal values for resource requests. It will watch a specified Deployment
      and automatically adjust the resource requests for its Pods based on what they
      actually use. It has a dry-run mode that will just make suggestions, without actually
      modifying the running Pods, and this can be helpful.

-- A good rule of thumb is that nodes should be big enough to run at least five of your
      typical Pods, keeping the proportion of stranded resources to around 10% or less. If
      the node can run 10 or more Pods, stranded resources will be below 5%.
      The default limit in Kubernetes is 110 Pods per node. Although you can increase this
      limit by adjusting the --max-pods setting of the kubelet, this may not be possible
      with some managed services, and it’s a good idea to stick to the Kubernetes defaults
      unless there is a strong reason to change them.
-- The Pods-per-node limit means that you may not be able to take advantage of your
      cloud provider’s largest instance sizes. Instead, consider running a larger number of
      smaller nodes to get better utilization. For example, instead of 6 nodes with 8 vCPUs,
      run 12 nodes with 4 vCPUs.

-- Larger nodes tend to be more cost-effective, because less of their resources are consumed 
      by system overhead. Size your nodes by looking at real-world utilization 
      figures for your cluster, aiming for between 10 and 100 Pods per node.

-- You can save on cloud costs by provisioning low-IOPS storage for workloads that don’t need so much
      bandwidth. On the other hand, if your application is performing poorly because it’s spending 
      a lot of time waiting for storage I/O, you may want to provision more IOPS to handle thi


-- the scheduler never moves Pods from one node to another unless they are restarted for 
      some reason. Also, the scheduler’s goal of placing workloads evenly across nodes is 
      sometimes in conflict with maintaining high availability for individual services.


• Kubernetes allocates CPU and memory resources to containers on the basis of requests and limits.
• A container’s requests are the minimum amounts of resources it needs to run. Its
    limits specify the maximum amount it’s allowed to use.
• Minimal container images are faster to build, push, deploy, and start. The smaller
    the container, the fewer the potential security vulnerabilities.
• Liveness probes tell Kubernetes whether the container is working properly. If a
    container’s liveness probe fails, it will be killed and restarted.
• Readiness probes tell Kubernetes that the container is ready and able to serve
    requests. If the readiness probe fails, the container will be removed from any
    Services that reference it, disconnecting it from user traffic.
• PodDisruptionBudgets let you limit the number of Pods that can be stopped at
    once during evictions, preserving high availability for your application.
• Namespaces are a way of logically partitioning your cluster. You might create a
    namespace for each application, or group of related applications.
• To refer to a Service in another namespace, you can use a DNS address like this: SERVICE.NAMESPACE.
• ResourceQuotas let you set overall resource limits for a given namespace.
• LimitRanges specify default resource requests and limits for containers in a namespace.
• Set resource limits so that your applications almost, but don’t quite exceed them in normal usage.
• Don’t allocate more cloud storage than you need, and don’t provision high-
    bandwidth storage unless it’s critical for your application’s performance.
• Set owner annotations on all your resources, and scan the cluster regularly for unowned resources.
• Find and clean up resources that aren’t being utilized (but check with their owners).
• Reserved instances can save you money if you can plan your usage long-term.
• Preemptible instances can save you money right now, but be ready for them to vanish at 
    short notice. Use node affinities to keep failure-sensitive Pods away from preemptible nodes.