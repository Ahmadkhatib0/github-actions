
# check the pods in the kube-system namespace of the current active cluster:
  kubectl get pods -n kube-system

# cluster status
k cluster-info

k get nodes 

k describe node minikube 

minikube addons list

# create deployment
$ k create deployment echo --image=k8s.gcr.io/e2e-test-images/echoserver:2.5

# To expose our (created) pod as a service, type the following:
$ k expose deployment echo --type=NodePort --port=8080


# Exposing the service as type NodePort means that it is exposed to the host on some port. 
# But it is not the 8080 port we ran the pod on. Ports get mapped in the cluster. To access 
# the service, we need the cluster IP and exposed port:
$ minikube ip     # 172.26.246.89

# then 
$ k get service echo -o jsonpath='{.spec.ports[0].nodePort}'   # 32649

# then access the echo service, which returns a lot of information:
$ curl http://172.26.246.89:32649/hi




# create cluster using kind config file
kind create cluster --config kind-ha-multi-node-config.yaml --kubeconfig $TMPDIR/kind-ha-multi-node-config

# after creating we'll get 3-node cluster 
$ k get nodes --kubeconfig $TMPDIR/kind-ha-multi-node-config

# KinD has its own get nodes command, where you can see the load balancer:
$ kind get nodes --name ha-multi-node-cluster

# deploy our echo service on the KinD cluster: 
$ k create deployment echo --image=g1g1/echo-server:0.1 --kubeconfig $TMPDIR/kind-ha-multi-node-config

# expose 
$ k expose deployment echo --type=NodePort --port=7070 --kubeconfig $TMPDIR/kind-ha-multi-node-config

# check the exposed service (external ip won't be shown, so complete the upcomming steps)
$ k get svc echo --kubeconfig $TMPDIR/kind-ha-multi-node-config 


# Accessing Kubernetes services locally through a proxy

# First, we need to run the kubectl proxy command that exposes the API server, pods, and services on localhost:
$ k proxy --kubeconfig $TMPDIR/kind-ha-multi-node-config &

# Then, we can access the echo service through a specially crafted proxy URL that includes 
# the exposed port (8080) and NOT the node port:
$ http http://localhost:8001/api/v1/namespaces/default/services/echo:7070/proxy/yeah-it-works




# Creating a multi-node cluster with k3d 

# creating single node: (Creating a single-node cluster with k3d takes less than 20 seconds!)
$ time k3d cluster create

$ kubectl cluster-info

$ k3d cluster delete


# creating  multi-node clusters: 
$ time k3d cluster create --agents 3


### leader election 
Here is a snippet from a scheduler manifest that shows the use of the –leader-elect flag command:
- /bin/sh
- -c
- /usr/local/bin/kube-scheduler --master=127.0.0.1:8080 --v=2 --leader-elect=true 
    1>> /var/log/kube-scheduler.log 2>&1
    
Here is a snippet from a controller manager manifest that shows the use of the –leader-elect flag, - command:
- /bin/sh
- -c
- /usr/local/bin/kube-controller-manager --master=127.0.0.1:8080 --cluster-name=e2e-test-bburns
    --cluster-cidr=10.245.0.0/16 --allocate-node-cidrs=true --cloud-provider=gce
    --service-account-private-key-file=/srv/kubernetes/server.key
    --v=2 --leader-elect=true 1>>/var/log/kube-controller-manager.log 2>&1
    image: gcr.io/google\_containers/kube-controller-manager:fda24638d51a48baa13c35337fcd4793



# get serviceaccount details
k get serviceaccounts/custom-service-account -o json

# see the serviceaccount's secret , which includes a ca.crt file and a token:
$ kubectl get secret custom-service-account-token-vbrbm -o yaml

# create a new KinD cluster and store its credentials in a dedicated config file:
$ export KUBECONFIG=~/.kube/kind-config

# Using the kubectl can-i command, you can check what actions you can perform and even impersonate other users:
$ kubectl auth can-i create deployments
$ kubectl auth can-i create deployments --as jack

# rolesum. This plugin gives you a summary of all the permissions a user or service account has
$ kubectl rolesum job-controller -n kube-system

# create a secret object for the credentials:
$ kubectl create secret docker-registry the-registry-secret \
    --docker-server=<docker registry server> \
    --docker-username=<username> \
    --docker-password=<password> \
    --docker-email=<email>

To activate Pod Security Admission on a namespace you simply add a label to the target namespace:
  $ MODE=warn # One of enforce, audit, or warn
  $ LEVEL=baseline # One of privileged, baseline, or restricted
  $ kubectl label namespace/ns-1 pod-security.kubernetes.io/${MODE}: ${LEVEL}

# adding network policies to allow ingress to specific pods explicitly
$ k create -n ${NAMESPACE} -f deny-all-network-policy.yaml

# create kubernetes secrets
$ k create secret generic hush-hush --from-literal=username=tobias --from-literal=password=cutoffs

# To get the content of a secret you can use kubectl get secret:
$ k get secrets/hush-hush -o yaml

# The secret values are base64-encoded. You need to decode them yourself:
$ k get secrets/hush-hush -o jsonpath='{.data.password}' | base64 --decode

$ k create -f ch4-security/pod-with-secret.yaml
  pod/pod-with-secret created

# get the secret inside a pod
$ k exec pod-with-secret -- cat /mnt/hush-hush/username          # tobias

# run a pod in interactive mode in the custom-namespace namespace:
$ k run trouble -it -n custom-namespace --image=g1g1/py-kube:0.3 bash

# Listing pods in the custom-namespace
$ k get po -n custom-namespace

# create multitenent cluster using vcluster
$ vcluster create tenant-1

# list clusters 
$ k config get-contexts -o name

# list what’s running in the vcluster- tenant-1 namespace:
$ k get all -n vcluster-tenant-1 --context kind-kind

# list what  namespaces are in the virtual cluster:
$ k get ns --context vcluster_tenant-1_vcluster-tenant-1_kind-kind

$ k get ns new-ns --context vcluster_tenant-2_vcluster-tenant-2_kind-kind
# Error from server (NotFound): namespaces "new-ns" not found (so vcluster is fully isolated)

$ k get ns new-ns --context kind-kind
Error from server (NotFound): namespaces "new-ns" not found   (so vcluster is fully isolated)





