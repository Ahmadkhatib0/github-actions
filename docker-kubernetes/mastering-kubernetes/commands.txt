
# check the pods in the kube-system namespace of the current active cluster:
  kubectl get pods -n kube-system

# cluster status
k cluster-info

k get nodes 

k describe node minikube 

minikube addons list

# create deployment
$ k create deployment echo --image=k8s.gcr.io/e2e-test-images/echoserver:2.5

# To expose our (created) pod as a service, type the following:
$ k expose deployment echo --type=NodePort --port=8080


# Exposing the service as type NodePort means that it is exposed to the host on some port. 
# But it is not the 8080 port we ran the pod on. Ports get mapped in the cluster. To access 
# the service, we need the cluster IP and exposed port:
$ minikube ip     # 172.26.246.89

# then 
$ k get service echo -o jsonpath='{.spec.ports[0].nodePort}'   # 32649

# then access the echo service, which returns a lot of information:
$ curl http://172.26.246.89:32649/hi




# create cluster using kind config file
kind create cluster --config kind-ha-multi-node-config.yaml --kubeconfig $TMPDIR/kind-ha-multi-node-config

# after creating we'll get 3-node cluster 
$ k get nodes --kubeconfig $TMPDIR/kind-ha-multi-node-config

# KinD has its own get nodes command, where you can see the load balancer:
$ kind get nodes --name ha-multi-node-cluster

# deploy our echo service on the KinD cluster: 
$ k create deployment echo --image=g1g1/echo-server:0.1 --kubeconfig $TMPDIR/kind-ha-multi-node-config

# expose 
$ k expose deployment echo --type=NodePort --port=7070 --kubeconfig $TMPDIR/kind-ha-multi-node-config

# check the exposed service (external ip won't be shown, so complete the upcomming steps)
$ k get svc echo --kubeconfig $TMPDIR/kind-ha-multi-node-config 


# Accessing Kubernetes services locally through a proxy

# First, we need to run the kubectl proxy command that exposes the API server, pods, and services on localhost:
$ k proxy --kubeconfig $TMPDIR/kind-ha-multi-node-config &

# Then, we can access the echo service through a specially crafted proxy URL that includes 
# the exposed port (8080) and NOT the node port:
$ http http://localhost:8001/api/v1/namespaces/default/services/echo:7070/proxy/yeah-it-works




# Creating a multi-node cluster with k3d 

# creating single node: (Creating a single-node cluster with k3d takes less than 20 seconds!)
$ time k3d cluster create

$ kubectl cluster-info

$ k3d cluster delete


# creating  multi-node clusters: 
$ time k3d cluster create --agents 3


### leader election 
Here is a snippet from a scheduler manifest that shows the use of the –leader-elect flag command:
- /bin/sh
- -c
- /usr/local/bin/kube-scheduler --master=127.0.0.1:8080 --v=2 --leader-elect=true 
    1>> /var/log/kube-scheduler.log 2>&1
    
Here is a snippet from a controller manager manifest that shows the use of the –leader-elect flag, - command:
- /bin/sh
- -c
- /usr/local/bin/kube-controller-manager --master=127.0.0.1:8080 --cluster-name=e2e-test-bburns
    --cluster-cidr=10.245.0.0/16 --allocate-node-cidrs=true --cloud-provider=gce
    --service-account-private-key-file=/srv/kubernetes/server.key
    --v=2 --leader-elect=true 1>>/var/log/kube-controller-manager.log 2>&1
    image: gcr.io/google\_containers/kube-controller-manager:fda24638d51a48baa13c35337fcd4793



# get serviceaccount details
k get serviceaccounts/custom-service-account -o json

# see the serviceaccount's secret , which includes a ca.crt file and a token:
$ kubectl get secret custom-service-account-token-vbrbm -o yaml

# create a new KinD cluster and store its credentials in a dedicated config file:
$ export KUBECONFIG=~/.kube/kind-config

# Using the kubectl can-i command, you can check what actions you can perform and even impersonate other users:
$ kubectl auth can-i create deployments
$ kubectl auth can-i create deployments --as jack

# rolesum. This plugin gives you a summary of all the permissions a user or service account has
$ kubectl rolesum job-controller -n kube-system

# create a secret object for the credentials:
$ kubectl create secret docker-registry the-registry-secret \
    --docker-server=<docker registry server> \
    --docker-username=<username> \
    --docker-password=<password> \
    --docker-email=<email>

To activate Pod Security Admission on a namespace you simply add a label to the target namespace:
  $ MODE=warn # One of enforce, audit, or warn
  $ LEVEL=baseline # One of privileged, baseline, or restricted
  $ kubectl label namespace/ns-1 pod-security.kubernetes.io/${MODE}: ${LEVEL}

# adding network policies to allow ingress to specific pods explicitly
$ k create -n ${NAMESPACE} -f deny-all-network-policy.yaml

# create kubernetes secrets
$ k create secret generic hush-hush --from-literal=username=tobias --from-literal=password=cutoffs

# To get the content of a secret you can use kubectl get secret:
$ k get secrets/hush-hush -o yaml

# The secret values are base64-encoded. You need to decode them yourself:
$ k get secrets/hush-hush -o jsonpath='{.data.password}' | base64 --decode

$ k create -f ch4-security/pod-with-secret.yaml
  pod/pod-with-secret created

# get the secret inside a pod
$ k exec pod-with-secret -- cat /mnt/hush-hush/username          # tobias

# run a pod in interactive mode in the custom-namespace namespace:
$ k run trouble -it -n custom-namespace --image=g1g1/py-kube:0.3 bash

# Listing pods in the custom-namespace
$ k get po -n custom-namespace

# create multitenent cluster using vcluster
$ vcluster create tenant-1

# list clusters 
$ k config get-contexts -o name

# list what’s running in the vcluster- tenant-1 namespace:
$ k get all -n vcluster-tenant-1 --context kind-kind

# list what  namespaces are in the virtual cluster:
$ k get ns --context vcluster_tenant-1_vcluster-tenant-1_kind-kind

$ k get ns new-ns --context vcluster_tenant-2_vcluster-tenant-2_kind-kind
# Error from server (NotFound): namespaces "new-ns" not found (so vcluster is fully isolated)

$ k get ns new-ns --context kind-kind
Error from server (NotFound): namespaces "new-ns" not found   (so vcluster is fully isolated)

# you can explore resources as well as specific sub-resources and fields.
$ kubectl explain pod # | pod.spec | pod.spec.containers

# query for matching labels
$ k get po -n kube-system --show-labels
$ k get po -n kube-system -l k8s-app=kube-dns   # filter and list only the kube-dns pods
$ k get pods -l app=hue
$ k get po -o wide -l app=hue,service=reminders


# get the images that were used to create pods: 
$ k get pods -o jsonpath='{.items[*].spec.containers[0].image}' -l app=hue | xargs printf "%s\n"


# show the internal DNS name
$ kubectl exec hue-learn-68d74fd4b7-rw4qr -- nslookup hue-reminders.default.svc. cluster.local

# query a pod using dns Name 
$ kubectl exec hue-learn-68d74fd4b7-fh55c -- wget -q -O - hue-reminders.default.svc. cluster.local:8080

# show which node a pod is located in 
$ k get po trouble-shooter -o jsonpath='{.spec.nodeName}'

# taint our control plane node:
$ k taint nodes k3d-k3s-default-server-0 control-plane=true:NoExecute

# review the taint:
$ k get nodes k3d-k3s-default-server-0 -o jsonpath='{.spec.taints[0]}'
# {"effect":"NoExecute","key":"control-plane","value":"true"}

# configure a context for a namespace 
$ k config set-context k3d-k3s-restricted --cluster k3d-k3s-default

# apply the kustomize on the base directory
$ k kustomize base

# Kustomize generates: 
$ k kustomize overlays/staging

# deploy it (that was created using the kustomize.yaml) to the cluster:
$ k apply -k overlays/staging

$ k get jobs

$ k get pod hue-scheduler -o json | jq .spec.containers

$ k rollout status deployment/nginx-deployment
$ k get deployment nginx -o yaml | grep strategy -A 4

# dispaly resources usage
$ k top po

# creating autoscaling pods using kubectl
$ k autoscale deployment bash-loop --min=4 --max=6 --cpu-percent=50


$ k set image deployment/hue-reminders hue-reminders=g1g1/hue-reminders:3.0
$ k rollout history deployment hue-reminders

# Kubectl also offers a convenient imperative command that can achieve the same result:
$ kubectl scale deployment YOUR_DEPLOYMENT –replicas=6

# You can watch the status of the deployment with the following:
$ kubectl get pods -w
# To watch all resources using a single command (which kubectl can’t do by itself), 
  the Linux watch command is convenient:
$ watch -d kubectl get deploy,hpa,pods 

# get the forwarding-rule info (if you are using matrics with GKE for example)
$ kubectl get ingress -o =jsonpath="{.items[0].metadata.annotations['ingress\.kubernetes\.io\/forwarding-rule']}"

$ kubectl rollout restart deployment DEPLOYMENT_NAME

$ minikube create --nodes 3

# In this example, spot=true is the name we gave to the taint, and is used later when
# marking Pods as able to tolerate this taint
$ kubectl taint nodes NODE_NAME spot=true:NoSchedule
$ kubectl taint nodes NODE_NAME spot-

# In GKE Standard, you can taint node pools when you create them. For example, you if you’re 
# creating a spot node pool, you can configure all the nodes to be tainted as follows
$ gcloud container node-pools create NODE_POOL_NAME --cluster CLUSTER_NAME --spot --node-taints spot=true:NoSchedule

$ kubectl taint nodes minikube-m02 group=1:NoSchedule #C
$ kubectl label node minikube-m02 group=1 #C
$ kubectl taint nodes minikube-m03 group=2:NoSchedule #D
$ kubectl label node minikube-m03 group=2 #D

#C taint and label the m02 node for group 1
#D taint and label the m03 node for group 2


$ kubectl patch storageclass standard-rwo -p 
  '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}'

# get quota
$ k get quota

# get the status of an installed chart 
$ helm status -n monitoring prometheus | grep STATUS
$ helm list -n monitoring

# Helm stores the release information in a secret:
$ kubectl describe secret sh.helm.release.v1.prometheus.v1 -n monitoring

ind all the Helm releases across all namespaces
$ helm list -A

# n list all the secrets that have the owner=helm label:
$ kubectl get secret -A -l owner=helm

# extract the release of an installed helm chart 
$ kubectl get secret sh.helm.release.v1.prometheus.v1 -n monitoring -o jsonpath=
  '{. data.release}' | base64 --decode | base64 --decode | gunzip > prometheus.v1.json

# extracting just the manifests using the following command:
$ kubectl get secret sh.helm.release.v1.prometheus.v1 -n monitoring -o jsonpath= 
  '{.data.release}' | base64 --decode | base64 --decode | gunzip | jq .manifest -r

# Customizing a chart (use this To learn about possible customizations)
$ helm show values prometheus-community/prometheus | wc -l
$ helm show values prometheus-community/prometheus | head -n 20

# If you want to customize any part of the Prometheus installation, then save the values to a file, 
  make any modifications you like, and then install Prometheus using the custom values file:
$ helm install prometheus prometheus-community/prometheus --create-namespace -n monitoring -f custom-values.yaml

# check the current values of our prometheus installation: 
$ helm get values prometheus -n monitoring


# Upgrading and rolling back a release by disable the alert manager by upgrading and passing a new value: 
$ helm upgrade --set alertmanager.enabled=false prometheus prometheus-community/prometheus -n monitoring
$ k get deployment -n monitoring            # check if it was removed
$ helm get values prometheus -n monitoring  # check the custom values (will show: enabled: false)

# if we want to reuse the alertmanager again, first show the installation history:
$ helm history prometheus -n monitoring

# Let’s roll back to revision 1               # (or whatever you want)
$ helm rollback prometheus 1 -n monitoring
$ helm history prometheus -n monitoring       # re-check

# Deleting a release
$ helm list -n monitoring        # first see releases
$ helm uninstall prometheus -n monitoring
$ helm list -n monitoring        # check it was uninstalled (notice the monitoring ns remained)


# create your ownn chart 
$ helm create cool-chart

# Once you have edited your chart, you can package it into a tar.gz archive:
$ helm package cool-chart
$ helm lint cool-chart

# update dependencies 
$ helm dep up cool-chart

# set tags and condition values from the command line too when installing a chart
  (this setting tags process will deside wither to enable a chart on not)
$ helm install --set subchart2.enabled=false

# use the helm template command to see the result of a helm template:
$ helm template food food-chart

# Another good way of debugging is to run install with the --dry-run flag. 
  It provides additional information:
$ helm install food food-chart --dry-run -n monitoring

# You can also override values on the command line:
$ helm template food food-chart --set favorite.drink=water

# install a locally created chart
$ helm install food food-chart -n monitoring
$ helm list -n monitoring           # check

# the internal DNS server ( which each /etc/resolv.conf file in each pod point to this dns service )
$ k describe svc -n kube-system kube-dns

# then Note that selector: k8s-app=kube-dns. Let’s find the pods that back this service:
$ k get po -n kube-system -l k8s-app=kube-dns

# you will see that the service is called kube-dns, but the pods have a prefix of coredns 
  (e.g coredns-64897985d-n4x5b). Interesting. Let’s check the image the deployment uses:
$ k get deploy coredns -n kube-system -o jsonpath='{.spec.template.spec.containers[0]}' | jq .image
  >>> "registry.k8s.io/coredns/coredns:v1.10.1"

# The reason for this mismatch is that, initially, the default Kubernetes DNS server was 
  called kube-dns. Then, CoreDNS replaced it as the mainstream DNS server due to its 
  simplified architecture and better performance.


# If a deployment or replica set creates multiple copies of the same pod, you can 
  query the logs of all pods in a single call by using a shared label:
$ k logs -l <label>


# see the node status and if it’s ready using this command:
$ k describe no kind-control-plane | grep Conditions -A 6



