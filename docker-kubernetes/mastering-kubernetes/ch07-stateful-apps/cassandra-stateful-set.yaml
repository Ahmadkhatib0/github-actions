---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: cassandra
  labels:
    app: cassandra
spec:
  serviceName: cassandra
  replicas: 3
  # The term “replicas” for the pods is an unfortunate choice because the pods are not replicas of each
  # other. They share the same pod template, but they have a unique identity, and they are responsible for
  # different subsets of the state in general. This is even more confusing in the case of Cassandra, which
  # uses the same term, “replicas,” to refer to groups of nodes that redundantly duplicate some subset of
  # the state (but are not identical, because each can manage an additional state too).
  selector:
    matchLabels:
      app: cassandra
  template:
    metadata:
      labels:
        app: cassandra
    spec:
      terminationGracePeriodSeconds: 1800
      containers:
        - name: cassandra
          image: gcr.io/google-samples/cassandra:v14
          imagePullPolicy: Always
          ports:
            - containerPort: 7000
              name: intra-node
            - containerPort: 7001
              name: tls-intra-node
            - containerPort: 7199
              name: jmx
            - containerPort: 9042
              name: cql
          # The resources section specifies the CPU and memory needed by the container. This is critical
          # because the storage management layer should never be a performance bottleneck due to CPU or
          # memory. Note that it follows the best practice of identical requests and limits to ensure the
          # resources are always available once allocated:
          resources:
            limits:
              cpu: '500m'
              memory: 1Gi
            requests:
              cpu: '500m'
              memory: 1Gi
          # Cassandra needs access to inter-process communication (IPC), which the container
          # requests through the security context’s capabilities:
          securityContext:
            capabilities:
              add:
                - IPC_LOCK
          # The lifecycle section runs the Cassandra nodetool drain command to make sure data on the node
          # is transferred to other nodes in the Cassandra cluster when the container needs to shut down. This is
          # the reason a 30-minute grace period is needed. Node draining involves moving a lot of data around:
          lifecycle:
            preStop:
              exec:
                command:
                  - /bin/sh
                  - -c
                  - nodetool drain
          env:
            - name: MAX_HEAP_SIZE
              value: 512M
            - name: HEAP_NEWSIZE
              value: 100M
            # The CASSANDRA_SEEDS variable is set to the headless service, so a Cassandra
            # node can talk to seed nodes on startup and discover the whole cluster
            - name: CASSANDRA_SEEDS
              value: 'cassandra-0.cassandra.default.svc.cluster.local'
            - name: CASSANDRA_CLUSTER_NAME
              value: 'K8Demo'
            - name: CASSANDRA_DC
              value: 'DC1-K8Demo'
            - name: CASSANDRA_RACK
              value: 'Rack1-K8Demo'
            - name: CASSANDRA_SEED_PROVIDER
              value: io.k8s.cassandra.KubernetesSeedProvider
            # POD_IP is interesting because it utilizes the Downward API to populate its value
            # via the field reference to status.podIP:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
          readinessProbe:
            exec:
              command:
                - /bin/bash
                - -c
                - /ready-probe.sh
            initialDelaySeconds: 15
            timeoutSeconds: 5
          volumeMounts:
            - name: cassandra-data
              mountPath: /var/lib/cassandra
  volumeClaimTemplates:
    - metadata:
        name: cassandra-data
        annotations:
          volume.beta.kubernetes.io/storage-class: fast
      spec:
        accessModes: ['ReadWriteOnce']
        resources:
          requests:
            storage: 1Gi
# The last part is the volume claim templates. In this case, dynamic
# provisioning is used. It’s highly recommended to use SSD drives for Cassandra storage, especially its
# journal. The requested storage in this example is 1 GiB. I discovered through experimentation that
# 1–2 TB is ideal for a single Cassandra node. The reason is that Cassandra does a lot of data shuffling
# under the covers, compacting and rebalancing the data. If a node leaves the cluster or a new one joins
# the cluster, you have to wait until the data is properly rebalanced before the data from the node that
# left is properly redistributed or a new node is populated.
# Note that Cassandra needs a lot of disk space to do all this shuffling. It is recommended to have 50%
# free disk space. When you consider that you also need replication (typically 3x), then the required
# storage space can be 6x your data size. You can get by with 30% free space if you’re adventurous and
# maybe use just 2x replication depending on your use case. But don’t get below 10% free disk space,
# even on a single node. Cassandra will simply get stuck and will be unable
# to compact and rebalance such nodes without extreme measures.
