Designing Good Health Checks: 
  When creating the HTTP endpoints to implement these checks, it’s important to take into
  account any external dependencies being tested. Generally, you don’t want external
  dependencies to be checked in the liveness probe, rather it should test only whether the
  container itself is running (assuming your container will retry the connections to its external
  connections). This is because there’s not really any value in restarting a container that’s
  running just fine, and only because it can’t connect to another service which is having
  trouble. This could cause unnecessary restarts that creates churn and could lead to cascading
  failures, particularly if you have a complex dependency graph

Since the liveness is only testing whether or not the server is responding, the result can
  and should be extremely simple, generally just a HTTP 200 status response, even one with
  no response body text. If the request can get through to the server code, then it must be
  “live”, and this is good enough.
For Readiness probes on the other hand, it’s generally desirable that they test their
  external dependencies (like a database connection). This is useful because if you have say 3
  replicas of a Pod, and only 2 of them can connect to your database, it makes sense to only
  have those 2 fully functional pods in the load balancer rotation. One way to test the
  connection is to lookup a single row from the database in your readiness check.

MAXSURGE
  “maxSurge” governs how many more Pods you’re willing to create during the rollout. For example, 
  if you set a replica count of 5, and a maxSurge of 2, then it may be possible to have 7 Pods 
  (of different versions) scheduled. The tradeoff is that the higher this number is, the faster 
  the rollout will complete, but the more resources it will (temporarily) use. If you’re highly 
  optimizing your costs, you could set this to 0. Alternatively, for a large deployment you could 
  temporarily increase the resources available in your cluster during the rollout by adding nodes, 
  and removing them when the rollout is complete.
  
MAXUNAVAILABLE
  “maxUnavailable” sets the maximum number of Pods that can be unavailable during updates
  (percentage values are also accepted, and are rounded down to the nearest integer). If you’ve tuned 
  your replica count to handle your expected traffic, you may not want to set this value much higher 
  than zero, as your service quality could degrade during updates. The trade off here is that the 
  higher the value, the more Pods can be replaced at once and the faster the rollout completes, 
  while reducing the number of Ready pods temporarily that are able to process traffic.
  Given that a rollout could coincide with another event that lowers availability like a node
  failure, for production workloads I would recommend setting this to 0. The caveat is that if
  you set it to 0, and your cluster has no schedulable resources, the rollout will get stuck and
  you will see Pods in the “Pending” state until resources become available. When the
  maxUnavailable is 0, maxSurge cannot also be zero (the system needs to surge, i.e. add
  create new pods, that are by definition not ready as they are booting).

RECOMMENDATION
  Rolling update is a good go-to strategy for most services. For production services,
  maxUnavailable is best set to 0. maxSurge should be at least 1, or higher if you have
  enough spare capacity and want faster rollouts.

Replacement Strategy ( type: Recreate )
  Another approach, some might say the old fashion approach, is to cut the application over directly—delete 
  all Pods of the old version, and schedule replacements of the new version. Unlike the other strategies 
  discussed here, this is not zero-downtime. It will almost certainly result in some unavailability. 
  With the right readiness checks in place, this downtime could be as short as the time to boot the 
  first Pod, assuming it can handle the client traffic at that moment in time. The benefit of this 
  strategy is does not require compatibility between the new version and the old version 
  (since the two versions won’t be running at the same time), nor does it require any additional 
  compute capacity at all (since it’s a direct replacement).
  For development and staging, this strategy may be useful to avoid needing to slightly
  overprovision compute capacity to handle rolling updates, and for its speed, but otherwise
  should generally be avoided.

Blue / Green Strategy
The benefits of this strategy is:
    • Only one version of the app is running at a time, for a consistent user experience
    • The rollout is fast (within seconds)
    • Rollbacks are similarly fast
The drawbacks are:
    • Temporarily consumes double the compute resources
    • Not supported directly by Kubernetes Deployments.
This is an advanced rollout strategy, popular with large deployments. There are often several
  other processes included. For example, when the new version is ready—it can be tested first
  by a set of internal users, followed by a percentage of external traffic prior to the 100% cut-
  over—a process known as canary analysis. After the cut-over, there is often a period of time
  where the new version continues to be evaluated, prior to the old version being scaled down
  (this could last days). Of course, keeping both versions scaled up doubles the resource
  usage, with the trade off that near-instant rollbacks are possible during that window.

Pod Scheduling
  The Kubernetes scheduler performs a resource-based allocation of Pods to Nodes, and is really the brains of 
  the whole system. When you submit your configuration to Kubernetes , it’s the scheduler that does the heavy 
  lifting of finding a Node in your cluster with enough resources, and tasks the Node with booting and running 
  the containers in your Pods.

This recreation of Pods due to node failures by the scheduler is separate behavior to the Pod restarts. Pod 
  restarts due to liveness or readiness failures are handled locally on the node by the kubelet, whereas the 
  Scheduler is responsible for monitoring the health of the nodes, and reallocating Pods when issues are detected.
  It is the scheduler that has the task of finding the right place in your cluster to fit the Pod, based on 
  its resource requirements and any other placement requirements. Any Pods that can’t be placed on the cluster 
  will have the status “Pending” .

Quality of Service
  Having limits higher than requests, or not set at all, introduces a new problem, and that is what to do when 
  all these Pods are consuming too many resources (most commonly, too much memory), and they need to be evicted 
  to reclaim the resource. Kubernetes performs a stack rank of importance when choosing which pods to remove 
  first. Kubernetes traditionally classified Pods into 3 quality of service classes in order to rank
  their priority. These are no longer directly used in the eviction algorithm,

GUARANTEED CLASS
  Guaranteed class pods are where the limits are set equal to the requests. This is the most
  stable configuration as the pod is guaranteed the resources it requested, no more, no less. If
  your Pod has multiple containers, they all must meet this requirement for the Pod to be
  considered Guaranteed.

BURSTABLE CLASS
  Burstable class Pods on the other hand have limits set higher than requests, and are able to “burst” temporarily 
  provided there are resources are available (i.e. from other Pods not using all of their requests, or unallocated 
  space on the node). You need to be careful with these Pods as there can be some unforeseen consequences such 
  as accidently relying on the bursting. Say a Pod lands on an empty node and can burst to its heart’s content. 
  Then sometime later, it gets rescheduled onto another node with less resources, the performance will now be 
  different. So, it’s important to test burstable pods in a variety of conditions.

BEST EFFORT
  Pods without any requests or limits set are considered “best effort” and are scheduled
  wherever Kubernetes wishes. This is the lowest of the classes, and I strongly recommend
  against using this pattern. You can achieve a similar result with the burstable class by setting
  really low requests, and that is more explicit than just closing your eyes and hoping for the best.

Evictions, Priority and Preemption
  In times of resource contention (too many Pods trying to burst at once), Kubernetes will reclaim resource 
  by removing (through a process known as evicting) pods that are using resources beyond their requested 
  allocation. This is why it’s so important to have a Pod’s resources adequately specified.

EVICTION
  Guaranteed class pods are never evicted in times of resource contention, so for a bulletproof
  deployment, always set your Pods’ limits equal to their requests to define them as guaranteed. 

When looking for Pods to evict, Kubernetes first considers those Pods which are using
  more resources than their requests, sorts them by their priority number, and finally by how
  many more resources (of the resource in contention) that the Pod is using beyond what it
  requested. By default, all Pods have the same priority number (zero). When all Pods have
  the same priority, the amount of usage above requests is what’s used to rank

“EVICTED” ERROR STATUS
  If you query your Pods and you see a status of “Evicted” it indicates that the scheduler evicted your pod 
  because it was using more resources than it requested. To resolve, increase the resources requested by your 
  containers, and review whether your need to add more compute capacity to your cluster as a result.

PREEMPTION
  When used by itself, priority is useful to rank workloads so that more important workloads
  are scheduled first and evicted last. There can be a situation however where the cluster does
  not have enough resources for a period of time and high-priority Pods are left stuck in
  Pending while low priority ones are already running.
  If you’d rather have higher priority workloads proactively bump lower priority ones rather
  than waiting for capacity to free up, you can add preemption behavior by changing the
  preemptionPolicy field in your PriorityClass

Priority and preemption really come into play when you’re juggling many deployments
  and looking to save money by squeezing every ounce of compute out of your cluster by
  overcommitting where you need a way to signal which of your Pods are more important to
  resolve the resource contention. I wouldn’t recommend starting with this design as you’re
  just adding complexity. The simpler way to get started is to allocate enough resources to
  schedule all your workloads amply, and fine tune things later to squeeze some more
  efficiency out of your cluster.

Kubernetes ships with a resource usage monitoring tool out of the box kubectl top. You can use it to view 
  the resources used by Pods and nodes. We’ll be focusing on Pods as that’s what we need to know to set 
  the right resource request. First, deploy your Pod with an excessively high resource request. This may even 
  be a Pod that’s already in production—after all, it’s generally OK for performance (though not always for budget) 
  to overestimate your resources needed. The goal of this exercise is to start high, observe the Pod’s actual 
  usage, then pair the requests back to provision the right resources and avoid wastage.

When Memory Leaks are OK
  Instagram famously 2 disabled the garbage collection in Python for a 10% CPU improvement. While this is probably
  not for everyone, it’s an interesting pattern to consider. Does it really matter if a process gets bloated over 
  time and is rebooted, if it all happens automatically, and there are thousands of replicas? Maybe not. Kubernetes 
  automatically restarts crashed containers (including when that crash is due to the system removing them due 
  to an out of memory condition), making it fairly easy to implement such a pattern. I wouldn’t recommend this 
  without thorough investigation, but I do think it indicates that if your application has a slow leak that it may not
  be your highest priority bug to fix. Importantly, you need to make sure you at least give the container enough 
  resources to boot and run for a time, otherwise you could get caught in a OOMKill crashloop which is no fun 
  for anyone. Having enough replicas is also important to avoid a user visible failure.

Using the data you gathered, find the lower bound by looking at the memory usage of your
  Pod under load, and add a reasonable buffer (at least 10%). With this example data, would
  have picked 505MB * 1.1 = 555Mb. You know it’s enough to run the Pod under load for at
  least an hour, with a bit to spare. Depending on your budget, and risk profile you can tune
  this number accordingly (the higher it is, the lower the risk, but higher the cost).
  So, requests need to at least cover the stable state of the Pod. What about the memory
  limit? Assuming your data is solid and covered all use-cases (i.e. there’s no high-memory
  code path that didn’t execute while you were observing), I wouldn’t set it too much higher
  than the 1-day value. Having an excessive limit (say, twice as high as the limit or greater)
  doesn’t really help much since you already measured how much memory your Pods need
  over the course of a day, and if you do have a memory leak it may be better for the Pod to
  simply be restarted by the system when the limit is hit, than to be allowed to grow excessively.
  An alternative is to simply set the limit equal to the request, for the guaranteed QoS
  class. This has the advantage of giving your Pod constant performance regardless of what
  else is running on the node. In this case, you should give the Pod a little extra resource
  buffer, since the Pod will be terminated the moment it exceeds its requested amount.

Setting CPU Requests and Limits
  Unlike Memory, CPU is compressible. What this means is that if you don’t get the CPU
  resources you need, the application just runs slower. Whereas if it doesn’t get the memory it
  needs, it will crash. You still likely want to give the application the CPU it needs, otherwise
  performance will decrease, but there’s not as much need to have a buffer as there is with memory

If you’re coming from an environment where installations of your application were
  expensive, either in monetary cost for servers, or time to configure instances, your
  application will likely have a lot of internal concurrency configured through the use of threads
  and/or forks, often described as the number of “workers” used to handle incoming requests concurrently.
  Concurrent workers still have advantages in the Kubernetes world due to their resource
  efficiency. I wouldn’t take a Pod that currently had 10 workers, and instead deploy 10 replicas with 1 worker 
  each. The container’s internal concurrency is very efficient memory wise, as forks share some of the memory 
  used by the application binary, and threads share even more. CPU is also pooled between workers which is 
  useful as a typical web application spends a lot of time waiting on external dependencies, meaning there 
  is often spare capacity to handle many requests at once.

Balancing the benefits of workers is the fact that the more replicas of a Pod you have, the
  more durable it is. For example, if you have 2 replicas of a pod, with 18 workers each to handle a total of 36 
  concurrent connections, then if one of those Pods were to crash (or be restarted because it failed the health check), 
  half your capacity would be offline before the Pod restarts. A better approach might be to have 6 Pods replicas 
  with 6 workers each, still maintaining some inter-container concurrency while adding some redundancy.

Summary
• The Kubernetes scheduler lies at the core of the system and does the heavy lifting of
  finding the right home for your deployment’s Pods on your infrastructure.
• The scheduler will try to fit as many containers as it can on a given node, provided
  Pods have resource requests set appropriately
• Kubernetes uses the Pod’s resource requests and limits to govern how resources are allocated, 
  overcommitted, and reclaimed
• Overcommitting resources using bursting can save resources but introduces performance variability
• The specification of requests and limits by your workloads sets the quality of service they receive
• When designing your workloads, there is an availability/resource-usage trade-off
  between the replica count and the Pod’s internal thread/process worker count
• Most platforms enable Pod spreading by default, to ensure that replicas are not generally placed on the same node, 
  thus avoiding a single point of failure. Make sure you have a few nodes in your cluster to achieve higher availability.

Scaling Pods and Nodes:
  In the case of GKE, if you use Autopilot, nodes are provisioned automatically, For GKE’s Standard mode of 
  operation, the command looks like this:
    gcloud container clusters resize cluster-name --node-pool pool-name --num-nodes $NODE_COUNT
  Scaling down is performed with the same commands. When you scale down the nodes, depending on your provider, 
  you should be able to run the same command as to scale up, and the cluster will first cordon and drain the 
  nodes (to prevent new Pods being scheduled on them, and give running Pods’ time to shutdown gracefully and 
  be re-created on other nodes). Alternatively, you can manually cordon, drain, and remove nodes with the 
  following commands:
    kubectl get nodes # get a list of node names
    kubectl cordon node $NODE_NAME # prevent scheduling on the node
    kubectl drain node $NODE_NAME # remove running pods from the node
    kubectl delete node $NODE_NAME

Many HTTP services spend a lot of time waiting on external services like databases. These deployments may 
  need to scale using other metrics like the number of requests per second hitting the service, rather 
  than the CPU utilization. Kubernetes offers two built in metrics: CPU and memory. It doesn’t directly 
  support metrics like requests per second, but it can be configured by using custom and external metrics 
  exposed by your monitoring service

OBSERVING AND DEBUGGING
  To see what the HPA is doing, you can run kubectl describe hpa. Pay particular attention
  to the “ScalingActive” condition. If it is `False`, then it likely means that your metric is not
  active which can be for a number of reasons: a) the metric adapter wasn’t installed (or isn’t
  authenticated), b) your metric name or selector is wrong, or c) there just isn’t any metrics
  available yet. Note that even with the correct configuration, you will see ‘False’ when there
  is no data (for example there are no requests), so be sure to send some requests to the
  endpoint and wait a minute or two for the data to come through before investigating further.

Spare Capacity with Cluster Autoscaling
  One of the drawbacks of autoscaling nodes compared to manually adding nodes is that sometimes the Autoscaler 
  can tune things a little too well and result in no spare capacity. This can be great for keeping costs down, 
  but it makes it slower to start new Pods, as capacity needs to be provisioned before the Pod can startup.
  Adding new Nodes then starting the Pod is slower than adding new Pods to existing Nodes. Nodes have to be 
  provisioned and booted, while Pods that get scheduled onto existing nodes just have to pull the container 
  and boot (and if the container is already in the cache, they can even start booting right away).
  One way to solve both these problems, while still keeping your autoscaler is to use a low
  priority balloon pod. This is a Pod that does nothing itself, other than to reserve capacity
  (keeping additional nodes up and running on standby). This Pod’s priority is low so that when
  your own workloads scale up, they can preempt this pod and use the node capacity.
  

Internal services are a way to scale how you develop and service your application by splitting your application 
  into multiple smaller services. These individual services can be on different development cycles 
  (possibly by different teams) and use completely different languages and technology from each other. After all, 
  as long as you can containerize it, you can run it in Kubernetes. No longer do you need to worry whether 
  your application deployment platform can run what you need it to run.
  
ClusterIP gives you a virtual IP address in the Kubernetes cluster. This IP is addressable
  from any Pod within your cluster (like in this example, your main application). NodePort on
  the other hand reserves a high-level port number on each node in the cluster, allowing you
  to access it from any Pod using localhost, and the service IP. In both cases, Kubernetes
  provides the networking setup to proxy requests to the Pods that back the service. 

When a request is made from a Pod to a service over the ClusterIP or NodePort, that
  request is first handled by the networking glue on the node, which has an updated list from
  the Kubernetes control plane of every Pod that belongs to that service (and which nodes
  those Pods are on). It will pick one of the Pod IPs at random and route the request to that
  Pod via its node. Fortunately, all this happens quite seamlessly, your app can simply make a
  request such like HTTP GET using the IP of the service, and everything behaves as you’d expect.

Kubernetes offers Pods two
  ways to perform service discovery, using a DNS lookup, or an environment variable. The DNS lookup 
  works cluster-wide, while the environment variable is only for Pods within the same namespace.
  Kubernetes automatically creates an environment variable for each service and populates it
  with the Cluster IP, and makes this available in every Pod that is created after the service is
  created. The variable follows a naming conversion whereby our example robohash-internal
  service gets the environment variable ROBOHASH_INTERNAL_SERVICE_HOST.
  Rather than figuring out the correct conversion, you can view a list of all such
  environment variables available to your Pod by running the env command on you Pod with
  exec, like so:   $ kubectl exec robohash-6c96c64448-7fn24 -- env

In summary, environment variable discovery has a few advantages:
    •  Super fast (they are string constants)
    •  No dependency on other the DNS Kubernetes component
And some disadvantages:
    •  Only available to pods in the same namespace
    •  Pods must be created after the service is created

In summary, DNS-based service discovery has a few advantages:
    • Can be called from any namespace in the cluster
    • No ordering dependencies
And some disadvantages:
    • Slightly slower than using an environment variable (which is a constant)
    • Dependency on the internal DNS service

level 3 (L3) load balancer, which balances requests at the network level, and can work with a variety of 
  protocols (TCP, UDP, SCTP). You configure the Service with your desired protocol and port, and you get 
  an IP that will balance traffic over your Pods. If you expose a HTTP service over a LoadBalancer, you need 
  to implement your own TLS termination handling (configuring certificates, and running a HTTPS endpoint), and 
  all traffic to that endpoint will get routed to one set of Pods (based on the matchLabels rules). There is no
  option for exposing two or more separate services directly on the same load balancer (though one can proxy 
  requests to the other internally). When you are publishing a HTTP app specifically, you may get more utility 
  from a so-called level 7 (L7) load balancer, which balances at the HTTP request level and can do more fancy 
  things like terminate HTTPS connections (meaning it will handle the HTTPS details for you), and perform 
  path-based routing, so you can serve a single domain host with multiple services. In Kubernetes, a HTTP load
  balancer is created with an Ingress object. Ingress lets you place multiple internal services behind a single external IP with load
  balancing. You can direct HTTP requests to different backend services based on their URI path (/foo, /bar), 
  hostname (foo.example.com, bar.example.com), or both. The ability to have multiple services running on a 
  single IP (and potentially serving different paths under a single domain name) is unique to Ingress, because 
  if you’d exposed them with individual Services of type LoadBalancer like in the earlier chapter, the services 
  would have separate IP addresses, necessitating separate domains (e.g. one Service hosted on foo.example.com,
  and the other on bar.example.com).


Node Selectors
  The way you target specific node features from your pods is with node selection and/or node affinity. Node 
  selection and affinity are simply ways to express the desired labels (and therefore features) of the nodes 
  that your Pods require, Take for example a Pod that needs to run on an Arm node. Arm nodes are labelled with 
  the well-known label kubernetes.io/arch: arm64 (well-known labels are those that are defined in the open 
  source and should be constant across different providers). We can use a nodeSelector, or node affinity 
  to target that label, and ensure our Pod will only run on an Arm node

The advantage of the nodeAffinity, and the reason you would use it is that it allows for more expressive logic

Caution with preferred affinity
  This preferredDuringSchedulingIgnoredDuringExecution logic may sometimes yield surprising results. While the 
  preference ordering works when you have existing unallocated capacity on nodes, the way it interacts with 
  cluster autoscaling when there is no unallocated capacity of the preferred type and a new node is needed might 
  be contrary to what you prefer. For example, in the event that there is any unallocated capacity on existing 
  nodes in your cluster, even if it is of the dis-preferred type, Kubernetes will actually schedule the Pod there 
  first, before the platform kicks in to add new nodes of your preferred type. The reason for this is that the 
  Kubernetes scheduler, responsible for placing Pods on nodes and the platform Autoscaler (a common platform 
  component responsible for adding new nodes) are operating somewhat separately. The way a typical node Autoscaler 
  works at the platform level is to look for Pending pods that can be scheduled if more capacity was added. But 
  since the Kubernetes scheduler kicks in first and places the pod on the dis-preferred but available capacity, 
  the Autoscaler doesn’t have a chance to act.
  My advice when using a cloud provider is that you can generally just require the functionality you need, and 
  rely on the fact that they will have capacity to serve those needs. An alternative method to reduce the instances 
  of dispreferred capacity being used is to minimize the amount of dispreferred capacity that the workload can 
  use by separating it from other workloads in the cluster. To do this, you can use a required selector to a set 
  of nodes that are tainted, just like earlier in the chapter, except in this case the nodes are tainted just 
  to keep the workload separate.

Tainting Nodes to Prevent Scheduling by Default
  Another common requirement when you have a group of nodes that have special characteristics is that Pods 
  should not be scheduled by default on these nodes. Take Arm for example: since Arm is relatively new and 
  not all container images yet support it, you may want to configure your cluster so that Arm nodes will 
  not be used for scheduling by default, unless the workload expressly indicates support. Other examples 
  include when you have a Node with special hardware like a GPU that you need to reserve only for Pods that 
  will use this hardware, and when you have Spot compute that can be shutdown abruptly, which not all 
  workloads may respond well to.
  While you could annotate every other Pod to avoid such nodes using node anti-affinity (that is, a node 
  affinity rule with the NotIn operator), that is laborious, so Kubernetes has the ability to so-called “taint 
  the node” to prevent Pods being scheduled on it by default. How it works is that you “taint” nodes that have 
  special characteristics and shouldn’t be scheduled on by default, then you “tolerate” this taint in 
  the Podspec of just those workloads that are OK to run on these nodes.

When you’re operating a hosted Kubernetes service, it’s unlikely that you’ll be tainting
  nodes individually like in the above example. That’s because generally a taint applies to a
  group of nodes (ones that share the same characteristic like Arm, or Spot, or GPU), and
  because nodes are regularly replaced during upgrade or repair events. Look for the platform
  provider’s API that allows you to taint groups of nodes, so that the taint will be applied to all
  nodes in the group, and persist during upgrades.


Workload Separation: 
  Another use for taints, tolerations and node selectors is to separate workloads. So far the use cases for 
  node selection we’ve covered were around feature-based selection: requiring the Arm architecture, Spot compute, 
  GPU nodes and the like. Node selection isn’t limited to node features, and also can be used to separate workloads
  from each other on nodes. While you can use pod anti-affinity (covered in the next section) to prevent particular 
  pods being co-located, sometimes it helps just to keep workloads on their own dedicated groups of nodes.
  One requirement for this I heard was from a customer who uses a cluster for a lot of batch workloads, consisting 
  of a deployment of a coordinator that schedules the work, and the Pods for the workloads themselves. They 
  preferred to keep the Pods for these two roles separate, so that any autoscaling of the nodes for the work 
  Pods doesn’t impact that of the coordinator Pods. Another example is for the noisy neighbor problem, where 
  two Pods can potentially compete for resources on the node and would be better if separated. Yet another 
  example might be where you simply want your workload to land on a “fresh” node. If you give the workload 
  a fresh workload separation label/toleration then you’re forcing the platform to create a new node for it, 
  which is generally on the current version of the cluster and therefore less likely to be disrupted with 
  an upgrade in the near future.

Kubernetes has a built-in way called a “topology spread constraint”. A topology spread constraint aims to spread 
  your nodes across a failure domain, such as the node, or even a whole zone, and multiple can be specified, 
  so you can spread across both nodes and zones, or any other failure domains defined by your provider.

Pod anti-affinity (Avoiding Certain Pods)
  imagine you have a deployment for a backend service, and a separate deployment for a caching service, and 
  would prefer they be spread out. For this, you can use pod anti-affinity. This is simply throwing the pod
  affinity rule from the previous section into reverse so that the Pods will be scheduled on other nodes 
  (or the topology of your choice).

PLACEMENT RULES DON ’T APPEAR TO WORK
  If your placement rules don’t appear to work in testing, the first thing I’d suggest is ensure you are not 
  using any “soft” placement rules. These rules mean that the scheduler basically ignores your rule when it 
  can’t be satisfied, which isn’t so great for testing. It’s better to verify that all your rules are working 
  before relaxing them by changing them to soft rules. Use a small cluster with only a couple of nodes, no 
  soft rules, and you should be able to observe the effect of the placement features. Verify that the rules 
  are enforced by intentionally attempting to schedule Pods that would violate the rules. Their status should 
  be “Pending” because the constraints can’t be satisfied.

PODS ARE PENDING
  Pods that display as the “Pending” state mean that the scheduler can’t find a suitable place for them. 
  this error can occurr in the context of the cluster not having enough resources to place the Pod. Once you 
  configure your placement rules, it’s possible the Pod can’t be scheduled because the rules can’t be satisfied. 
  To find out what the reason is (i.e. which rule couldn’t be satisfied), describe the pod. Note, that you 
  need to do this at a Pod level – the deployment itself won’t show any error messages 
  (although it will indicate that the desired number of replicas isn’t met).

Using Persistent Volumes, you can attach stateful storage to any Kubernetes Pod. When it comes to 
  multi-replica deployments, just as Kubernetes offers Deployment as a high-level construct for managing a stateless 
  application, StatefulSet exists to provide high-level management of stateful applications. Just like nodes are 
  the Kubernetes representation of a virtual machine, Kubernetes has its own representation of disks as well

Kubernetes ships with some built in volume types, and others can be added by your platform administrator via 
  storage drivers. Some types you may encounter frequently are emptyDir, an ephemeral volume tied to the 
  lifecycle of the node, ConfigMap, which allows you specify files in Kubernetes manifests and present them 
  to your application as file on disk, and cloud provider disks for persistent storage.

CONFIGMAP VOLUME
  ConfigMap is a useful Kubernetes object. You can define key/value pairs and reference them in other object 
  declarations. You can also use them to store entire files! Typically these files would be configuration 
  files like my.cnf for MariaDB, httpd.conf for Apache, redis.conf for Redis and so on. You can mount the 
  ConfigMap as a volume, which allows the files it defines to be read from the container. ConfigMap volumes 
  are read-only. This technique is particularly useful for defining a configuration file for use by a public
  container image, as it allows you to provide configuration without needing to extend the image itself. For 
  example, to run Redis you can reference the official Redis image, and just mount your config file using 
  ConfigMap wherever Redis expects it, no need to build your own image just to provide this one file.
  
CLOUD PROVIDER VOLUMES
  More applicable for building stateful applications (where you don’t typically want to use ephemeral or 
  read-only volumes), is mounting disks from your cloud provider as volumes. Wherever you are running 
  Kubernetes, your provider should have supplied drivers into the cluster that allow you to mount 
  persistent storage, whether that’s NFS or block-based (often, both).


Persistent Volumes and Claims
  To provide a way to manage persistent volumes in a more platform-agnostic way, Kubernetes offers higher 
  level primitives around volumes, known as persistent volumes and persistent volume claims. Instead of 
  linking to the volume directly, the pod references a PersistentVolumeClaim object which defines the disk 
  resources that the Pod requires in platform-agnostic terms (for example: “1 gigabyte of storage”). The disk 
  resources themselves are represented in Kubernetes using a PersistentVolume object, much like 
  how Nodes in Kubernetes represent the virtual machine resource

If your provider supports dynamic provisioning, a PersistentVolume backed by a disk
  resource will be created to fulfil the storage requested by the PersistentVolumeClaim, after
  which the PersistentVolumeClaim and PersistentVolume will be bound together. The
  dynamic provisioning behavior of the PersisentVolume is defined through the StorageClass

When you have existing data that you wish to mount into a Pod. In that case, you create the PersistentVolumeClaim 
  and the PersistentVolume objects already pointing at each other, so they are bound immediately at birth.

Storage classes are way to describe the different types of dynamic storage that are available to be requested 
  from PersistentVolumeClaims, and how the volumes that are requested in this way should be configured

The benefit of using a Deployment even for a single-replica pod 
  is that if the pod is terminated it will be recreated.

The limitation of the deployment constructs like Deployment that all pods share the same specification 
  is an issue for ReadWriteOnce volumes. These traditional block storage volumes can only be mounted by 
  a single instance. This is OK when there is only one replica in your Deployment, but it means that if you 
  create a second replica, that Pod will fail to be created as the volume is already mounted. Fortunately, 
  Kubernetes has a high-level construct that makes our lives easier when we need multiple Pods where they 
  each get their own disk (a highly common pattern). Just like Deployment is a high-level construct for managing 
  (normally stateless) continuously running services, StatefulSet is a high-level construct for managing stateful 
  services. StatefulSet has a few helpful properties for building such services. You can define a volume template 
  instead of a referencing a single volume in the pod spec and Kubernetes will create a new persistent volume 
  claim for each pod which solves the issue when using Deployment, where each instance shared the same persistent 
  volume claim. StatefulSet assigns each pod a stable identifier which is linked to a particular persistent 
  volume claim and provides ordering guarantees for deployment, scaling and updates. Now you can create 
  multiple Pods and coordinate them by using this stable identifier to assign each a different role.


Deploying a Multi-Role StatefulSet
  The real power of stateful set comes into play when you need to have multiple pods. When
  designing an application that will use stateful set, pod replicas within the stateful set need to
  know about each other and communicate with each other as part of the stateful application
  design. This is the benefit though of using stateful set because each of the pods gets a unique
  identifier in a set known as the ordinal. You can use this uniqueness and guaranteed ordering
  to assign different roles to the different unique Pods in the set and associate the same
  persistent disk through updates and even deletion and recreate.

One thing to be very aware of is that commonly by default, volumes that Kubernetes creates are deleted 
  if you delete the associated bound PersistentVolumeClaim because they are configured with the Delete 
  retainPolicy. With this policy set in the StorageClass, deleting the StatefulSet doesn’t itself delete the 
  PersistentVolumeClaim, which is good (forcing admins to manually clean up the StatefulSet’s PersistentVolumeClaims 
  if they do wish to delete them, but avoiding accidental delete). But, deleting the PersistantVolumeClaim 
  objects will delete the underlying disk resources and it’s not that hard to do (e.g. by passing --all to 
  the relevant kubectl delete command).

You may be wondering why a separate construct is needed in Kubernetes to run something
  once, since stand-alone Pods could do that as well. While it’s true that you can schedule a Pod
  to perform a task and shutdown once its complete, there is no controller to ensure that the
  task actually completes. For example, if the Pod was evicted due to a maintenance event before
  it had a chance to complete. Job adds some useful constructs around the Pod to ensure that
  the task will complete (by rescheduling it if it failed or was evicted), as well as the potential to
  track multiple completions and parallelism.
  
If you use exec to perform maintenance tasks, then your task is sharing the resources with the Pod
  which isn’t great—the pod may not have enough resources to handle both, and you are impacting the 
  performance. By moving tasks to a Job, they get their own Pod, with their own resource allocation.

Background Processing Summary
• Deployments can be used to build a continuously running job queue, using a queue
  data structure like that offered by Redis for coordination.
• The background processing that many websites run to offload heavy requests would
  typically be run as a Deployment
• Kubernetes also has a dedicated Job object for running tasks
• Jobs can be used for one-off tasks, such as a manual maintenance task
• CronJob can be used to schedule Jobs to run, for example a daily cleanup task
• The Job object also supports batch jobs through the completions and parallel configuration
• Unlike a Deployment-based background queue, Job can be used to schedule work on a
  static queue, avoiding the need for a queue data structure
• Liveness checks are still relevant for Pods that process background tasks (to detect
  stuck/hung processes), and can be configured using an exec liveness check

  
